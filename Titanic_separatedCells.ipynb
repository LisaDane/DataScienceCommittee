{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LisaDane/DataScienceCommittee/blob/main/Titanic_separatedCells.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beginning"
      ],
      "metadata": {
        "id": "n133RYb34Heh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), cool 2D arrays\n",
        "\n",
        "# how to use google drive #\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')      #attaches itself to your google drive\n",
        "\n",
        "# takes the training data spreadsheet (csv) and puts it into two panda arrays #\n",
        "trainData = pd.read_csv(\"/content/drive/MyDrive/DS/train.csv\")\n",
        "testData = pd.read_csv(\"/content/drive/MyDrive/DS/test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj7zWI70sMcG",
        "outputId": "f5b306f1-fe6a-491d-cea8-6c9991dd1f71"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"The funtions bellow act on the data\"\"\"\n",
        "# just prints the data\n",
        "def printData():\n",
        "  print(f\"Train data=\\n{trainData}\")\n",
        "  print(f\"Test data=\\n{testData}\")"
      ],
      "metadata": {
        "id": "E44KeslCsVdD"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Currently working on"
      ],
      "metadata": {
        "id": "e-D4k3FP4RAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# makes the column Family column which is the sum of Sibsp (siblingings/spouses) and Parch (Parens/Children)\n",
        "def family(trainData, testData):\n",
        "  trainData[\"Family\"] = trainData[\"SibSp\"] + trainData[\"Parch\"]\n",
        "  testData[\"Family\"] = testData[\"SibSp\"] + testData[\"Parch\"]\n",
        "\n",
        "  print(\"   Created 'Family' column.\")\n",
        "\n",
        "  # makes the column Family column which is the sum of Sibsp (siblingings/spouses) and Parch (Parens/Children)\n",
        "def lastName(trainData, testData):\n",
        "  trainData[\"LastName\"] = trainData[\"Name\"].str.split(',').str[0]\n",
        "  testData[\"LastName\"] = trainData[\"Name\"].str.split(',').str[0]\n",
        "\n",
        "  print(\"   Created 'LastName' column.\")"
      ],
      "metadata": {
        "id": "QNWIzOB0sZ_2"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeFamCodes (trainData, testData):\n",
        "  trainNameDic = trainData[\"LastName\"].to_dict()\n",
        "  testNameDic = testData[\"LastName\"].to_dict()\n",
        "\n",
        "  i = 1\n",
        "  for name in trainNameDic:\n",
        "    famCode = \"\"\n",
        "    famCode += trainData[\"LastName\"].apply(lambda s: str(i) if s == name else \"\")\n",
        "\n",
        "    trainData[\"famCode\"] = famCode\n",
        "    i = i + 1\n",
        "\n",
        "  for name in testNameDic:\n",
        "    famCode = \"\"\n",
        "    famCode += testData[\"LastName\"].apply(lambda s: str(i) if s == name else \"\")\n",
        "\n",
        "    testData[\"famCode\"] = famCode\n",
        "    i = i + 1"
      ],
      "metadata": {
        "id": "sM-CgBErvfCe"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILLING in NULLS #\n",
        "def wrangleMean(trainData, testData):\n",
        "  trainData[\"Cabin\"] = trainData[\"Cabin\"].fillna('?')\n",
        "  testData[\"Cabin\"] = testData[\"Cabin\"].fillna('?')\n",
        "\n",
        "  trainData[\"Age\"] = trainData[\"Age\"].fillna(trainData[\"Age\"].mean())\n",
        "  testData[\"Age\"] = testData[\"Age\"].fillna(testData[\"Age\"].mean())\n",
        "\n",
        "  trainData[\"Fare\"] = trainData[\"Fare\"].fillna(trainData[\"Fare\"].mean())\n",
        "  testData[\"Fare\"] = testData[\"Fare\"].fillna(testData[\"Fare\"].mean())\n",
        "\n",
        "  print(\"   Nulls Filled.\")"
      ],
      "metadata": {
        "id": "T8Y0VLRYspVL"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with nulls in them #\n",
        "def wrangleDelNull(trainData, testData):\n",
        "  trainData.dropna(inplace=True)\n",
        "  testData.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "BfAvRz2qssMC"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# forces all numeric data to be withing 0 to 1\n",
        "#has optional param that acccepts a list of columns to ignore\n",
        "def wrangleNorm(trainData, testData, exclude=[]):\n",
        "  # change this to do all numeric columns!\n",
        "  tempDF = testData.select_dtypes(include=np.number)\n",
        "  columns = []\n",
        "\n",
        "    # this is for if you want to exclude a ceetain column (feature)\n",
        "  # it only adds colmuns that are not in th einclude list tot he columns list\n",
        "  for col in tempDF.columns:\n",
        "    if (col not in exclude):\n",
        "      columns = columns + [col]\n",
        "\n",
        "  #iterates through every numeric column\n",
        "  for col in columns:\n",
        "      trainData[col] = (trainData[col] - trainData[col].min()) / (trainData[col].max() - trainData[col].min())\n",
        "      testData[col] = (testData[col] - testData[col].min()) / (testData[col].max() - testData[col].min())\n",
        "\n",
        "  print(\"   Normalized numeric columns.\")"
      ],
      "metadata": {
        "id": "ktf3A6rGszf_"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The functions below display information about the data\"\"\"\n",
        "!pip install tensorflow_decision_forests\n",
        "import tensorflow_decision_forests as tfdf\n",
        "# This predicts importance of our features #\n",
        "def predictImportance(trainData):\n",
        "    tfDataSet = tfdf.keras.pd_dataframe_to_tf_dataset(trainData, label='Survived')\n",
        "\n",
        "    model = tfdf.keras.RandomForestModel()\n",
        "    model.fit(tfDataSet)\n",
        "\n",
        "    print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLMSBf1QtLzX",
        "outputId": "3e583fad-9061-46dc-f0df-f99c053cc751"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_decision_forests in /usr/local/lib/python3.10/dist-packages (1.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.5.3)\n",
            "Requirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (2.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (0.42.0)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (3.0.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2023.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# displays correlation data give a training data set #\n",
        "from pandas.plotting import scatter_matrix\n",
        "def displayCorrelations(trainData):\n",
        "                                          #Correlation values\n",
        "    print(f\"\\nCorrelation Summary:\\nPearson Coef:{trainData.corr(method = 'pearson')}\\n\\nKendall Coef:{trainData.corr(method = 'kendall')}\\n\\nSpearman Coef:{trainData.corr(method = 'spearman')}\\n\")\n",
        "\n",
        "    #big corrolation chart\n",
        "    trainData.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False)\n",
        "    plt.tight_layout(pad=2)\n",
        "    plt.show()\n",
        "    # histograms\n",
        "    trainData.hist()\n",
        "    plt.tight_layout(pad=2)\n",
        "    plt.show()\n",
        "    # scatter plot matrix\n",
        "    scatter_matrix(trainData)\n",
        "    plt.tight_layout(pad=.1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Qeu0qjc5tRZt"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# we probably dont need all these #\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "t7RrCD0ktWI8"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Models"
      ],
      "metadata": {
        "id": "RRdd_BBF6bAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This tests the models #\n",
        "def testModels(trainData,features):\n",
        "    print(f\"\\nTesting models based on: {features}\")\n",
        "    # what we want to predict #\n",
        "    y = trainData[\"Survived\"]\n",
        "\n",
        "    # Change Categorical to numeric #\n",
        "    X = pd.get_dummies(trainData[features])\n",
        "\n",
        "    names = [\"RandomForest200\",\n",
        "         \"LinearSVC\",\n",
        "         \"MultinomialNB\",\n",
        "         \"Logistic\",\n",
        "         \"KNN5\",\n",
        "         \"SVCLinear\",\n",
        "         \"SVCgamma2\",\n",
        "         \"DecisionTree5\",\n",
        "         \"RandomForest5-10\",\n",
        "         \"MLP-2000-1000-500-100\",\n",
        "         \"AdaBoost\",\n",
        "         \"HistGradientBoost\"\n",
        "        ]\n",
        "\n",
        "    abrevNames = [\"RF2\",\n",
        "         \"LSVC\",\n",
        "         \"MNB\",\n",
        "         \"Log\",\n",
        "         \"KNN5\",\n",
        "         \"SVCL\",\n",
        "         \"SVCg\",\n",
        "         \"DT5\",\n",
        "         \"RF51\",\n",
        "         \"MLP\",\n",
        "         \"AB\",\n",
        "         \"HGB\"\n",
        "        ]\n",
        "\n",
        "    models = [\n",
        "        RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
        "        LinearSVC(max_iter=1000, dual=False),\n",
        "        MultinomialNB(),\n",
        "        LogisticRegression(random_state=0),\n",
        "        KNeighborsClassifier(5),\n",
        "        SVC(kernel=\"linear\", C=0.025),\n",
        "        SVC(gamma=2, C=1),\n",
        "        DecisionTreeClassifier(max_depth=5),\n",
        "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "        #MLPClassifier(hidden_layer_sizes=(2000,1000,500,100)),\n",
        "        #MLPClassifier(hidden_layer_sizes=(100,50,10)),\n",
        "        AdaBoostClassifier(),\n",
        "        HistGradientBoostingClassifier(categorical_features=[])\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    i = 0\n",
        "    for model in models:\n",
        "        kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
        "        model_name = names[i]\n",
        "        i = i + 1\n",
        "        cv_results = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
        "        results.append(cv_results)\n",
        "        print('%s: %f (%f)' % (model_name, cv_results.mean(), cv_results.std()))\n",
        "\n",
        "    plt.boxplot(results, labels=abrevNames)\n",
        "    plt.title('Algorithm Comparison')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FNd6OXw2t7Ir"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The functions below predict and prepare predicitons for submissions\"\"\"\n",
        "# Only predicts on ONE model\n",
        "def outPrediction(trainData,testData):\n",
        "    # what we want to predict #\n",
        "    y = trainData[\"Survived\"]\n",
        "\n",
        "    # features that mater!! #\n",
        "    features = [\"Sex\",\"Fare\",\"Age\"]\n",
        "\n",
        "    # Change Categorical to numeric #\n",
        "    X = pd.get_dummies(trainData[features])\n",
        "    XTest = pd.get_dummies(testData[features])\n",
        "\n",
        "    model = HistGradientBoostingClassifier(categorical_features=[])\n",
        "    model.fit(X, y)\n",
        "\n",
        "    cv_results = cross_val_score(model, X, y, scoring='accuracy')\n",
        "    result = cv_results\n",
        "    name = \"Hist\"\n",
        "    print()\n",
        "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
        "    print()\n",
        "\n",
        "    predictions = model.predict(XTest)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "w_ToBJ6auBge"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outputPredictionsList(trainData,testData,features):\n",
        "    print(f\"   Using Features: {features}\")\n",
        "    # what we want to predict #\n",
        "    y = trainData[\"Survived\"]\n",
        "\n",
        "    # Change Categorical to numeric #\n",
        "    X = pd.get_dummies(trainData[features])\n",
        "    XTest = pd.get_dummies(testData[features])\n",
        "\n",
        "    models = [\n",
        "        RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0),\n",
        "        LinearSVC(max_iter=1500, dual=False),\n",
        "        MultinomialNB(),\n",
        "        LogisticRegression(random_state=0),\n",
        "        KNeighborsClassifier(5),\n",
        "        SVC(kernel=\"linear\", C=2),\n",
        "        SVC(gamma=0.5, C=2),\n",
        "        DecisionTreeClassifier(max_depth=15),\n",
        "        RandomForestClassifier(max_depth=15, n_estimators=30, max_features=3),\n",
        "        #MLPClassifier(hidden_layer_sizes=(2000,1000,500,100)),\n",
        "        #MLPClassifier(hidden_layer_sizes=(100,50,10)),\n",
        "        AdaBoostClassifier(),\n",
        "        HistGradientBoostingClassifier(categorical_features=[])\n",
        "    ]\n",
        "\n",
        "    predictionsList = []\n",
        "\n",
        "    for model in models:\n",
        "        model.fit(X, y)\n",
        "        prediction = model.predict(XTest)\n",
        "        predictionsList.append(prediction)\n",
        "\n",
        "    return np.array(predictionsList)"
      ],
      "metadata": {
        "id": "orB-elBZuKZR"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes each models predictions for each person and takes the average to round to the most likely outcome\n",
        "def outputConcensus(preditcionsList):\n",
        "  print(\"   Making Concensus...\")\n",
        "  concensus = np.mean(predictionsList, axis=0)\n",
        "  #print(concensus)\n",
        "  return (np.rint(concensus)).astype(int)"
      ],
      "metadata": {
        "id": "x08YzFs8uPXC"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an output dataframe and write it to csv file #\n",
        "def modelToCSV(testData,prediction):\n",
        "    output = pd.DataFrame({'PassengerId': testData.PassengerId, \"Survived\": prediction})\n",
        "    output.to_csv('submission.csv', index=False)\n",
        "\n",
        "    # just so we can see the submission in the console #\n",
        "    submission = pd.read_csv('submission.csv')\n",
        "    print(f\"CSV submission conents:\\n{submission}\")\n",
        "\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "7V-pLz1-uRQv"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METHOD CALLS"
      ],
      "metadata": {
        "id": "x94zxkSV36_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# METHOD CALLS #\n",
        "\n",
        "# WRANGLING #\n",
        "print('\\nWrangling...')\n",
        "wrangleMean(trainData, testData)\n",
        "#wrangleDelNull(trainData, testData)\n",
        "wrangleNorm(trainData, testData, exclude=[\"PassengerId\"])\n",
        "#printData()\n",
        "\n",
        "# FEATURE ENGINEERING #\n",
        "print('\\nCreating Features...')\n",
        "family(trainData, testData)\n",
        "lastName(trainData, testData)\n",
        "makeFamCodes (trainData, testData)\n",
        "\n",
        "#printData()\n",
        "\n",
        "# FEATURE SETS #\n",
        "features = [\"Sex\",\"Fare\",\"Age\",\"SibSp\",\"Parch\"]\n",
        "features1 = [\"Sex\",\"Fare\",\"Age\",\"Family\"]\n",
        "features2 = [\"Sex\",\"Fare\",\"Age\",\"Family\",\"Pclass\",\"Embarked\"]\n",
        "featuresALL = [\"Sex\",\"Fare\",\"Age\",\"Family\",\"Pclass\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",\"PassengerId\"]\n",
        "\n",
        "# TESTING #\n",
        "#predictImportance(trainData)\n",
        "#displayCorrelations(trainData)\n",
        "testModels(trainData,features)\n",
        "#testModels(trainData, features1)\n",
        "#testModels(trainData, features2)\n",
        "#testModels(trainData, featuresALL)\n",
        "\n",
        "# For  predictiing & submitting #\n",
        "print('\\nPredicting...')\n",
        "#prediction = outPrediction(trainData, testData)\n",
        "#predictionsList = outputPredictionsList(trainData,testData,features2)\n",
        "#print(predictionsList)\n",
        "#concensus = outputConcensus(predictionsList)\n",
        "#print(concensus)\n",
        "#modelToCSV(testData,concensus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yBi5qo8x2ET4",
        "outputId": "7962e631-1163-4ce8-c5d8-c5ea6c972b4f"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Wrangling...\n",
            "   Nulls Filled.\n",
            "   Normalized numeric columns.\n",
            "\n",
            "Creating Features...\n",
            "   Created 'Family' column.\n",
            "   Created 'LastName' column.\n",
            "\n",
            "Testing models based on: ['Sex', 'Fare', 'Age', 'SibSp', 'Parch']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-bed260b4414c>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#predictImportance(trainData)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#displayCorrelations(trainData)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtestModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#testModels(trainData, features1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#testModels(trainData, features2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-d681bcabd925>\u001b[0m in \u001b[0;36mtestModels\u001b[0;34m(trainData, features)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s: %f (%f)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         )\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \"\"\"\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
          ]
        }
      ]
    }
  ]
}